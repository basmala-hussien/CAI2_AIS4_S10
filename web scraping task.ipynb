{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dfb1e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraped successfully\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Fetch and parse HTML content\n",
    "# Define the website URL to scrape and send an HTTP GET request\n",
    "website  = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(website)\n",
    "\n",
    "# Parse the HTML response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Utility function to write data to CSV\n",
    "# Saves extracted data into a CSV file with specified headers and rows\n",
    "def write_csv(filename, headers, rows):\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "# Utility function to write data to JSON\n",
    "# Saves extracted data into a JSON file with formatted output\n",
    "def write_json(filename, data):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# 1. Extract Text Data\n",
    "# Extracts headings (h1, h2), paragraphs (p), and list items (li) from the HTML\n",
    "# Saves the extracted data into a CSV file\n",
    "def extract_text_data():\n",
    "    headings = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2'])]\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "    list_items = [li.get_text(strip=True) for li in soup.find_all('li')]\n",
    "    write_csv(\"Extract_Text_Data.csv\", [\"Headings\", \"Paragraphs\", \"List Items\"],\n",
    "              [[\", \".join(headings), \", \".join(paragraphs), \", \".join(list_items)]])\n",
    "\n",
    "# 2. Extract Table Data\n",
    "# Extracts rows and columns from an HTML table and saves them into a CSV file\n",
    "def extract_table_data():\n",
    "    table = soup.find('table')\n",
    "    if not table:  # If no table is found, exit the function\n",
    "        return\n",
    "    rows = [[cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])] for row in table.find_all('tr')]\n",
    "    write_csv(\"Extract_Table_Data.csv\", rows[0], rows[1:])\n",
    "\n",
    "# 3. Extract Product Information\n",
    "# Extracts product details from cards with a specific class and saves them into a JSON file\n",
    "def extract_product_information():\n",
    "    cards = soup.find_all(class_=\"book-card\")\n",
    "    products = []\n",
    "    for card in cards:\n",
    "        products.append({\n",
    "            \"Book Title\": card.find(class_=\"book-title\").get_text(strip=True) if card.find(class_=\"book-title\") else \"\",\n",
    "            \"Price\": card.find(class_=\"price\").get_text(strip=True) if card.find(class_=\"price\") else \"\",\n",
    "            \"Stock Availability\": card.find(class_=\"stock-status\").get_text(strip=True) if card.find(class_=\"stock-status\") else \"\",\n",
    "            \"Button Text\": card.find('button').get_text(strip=True) if card.find('button') else \"\"\n",
    "        })\n",
    "    write_json(\"Product_Information.json\", products)\n",
    "\n",
    "# 4. Extract Form Details\n",
    "# Extracts details of form inputs (name, type, and default value) and saves them into a JSON file\n",
    "def extract_form_details():\n",
    "    form = soup.find('form')\n",
    "    if not form:  # If no form is found, exit the function\n",
    "        return\n",
    "    inputs = form.find_all('input')\n",
    "    form_data = [{\"Field Name\": inp.get('name', \"\"), \"Input Type\": inp.get('type', \"\"), \"Default Value\": inp.get('value', \"\")} for inp in inputs]\n",
    "    write_json(\"Form_Details.json\", form_data)\n",
    "\n",
    "# 5. Extract Links and Multimedia\n",
    "# Extracts all links and the video URL (if available) and saves them into a JSON file\n",
    "def extract_links_and_multimedia():\n",
    "    links = [{\"Text\": a.get_text(strip=True), \"Href\": a.get('href', \"\")} for a in soup.find_all('a')]\n",
    "    video_link = soup.find('iframe').get('src') if soup.find('iframe') else \"\"\n",
    "    write_json(\"Links_and_Multimedia.json\", {\"Links\": links, \"Video Link\": video_link})\n",
    "\n",
    "# 6. Extract Featured Products\n",
    "# Extracts featured product details (id, name, price, colors) and saves them into a JSON file\n",
    "def extract_featured_products():\n",
    "    featured_products = soup.find_all(class_=\"featured-product\")\n",
    "    products = []\n",
    "    for product in featured_products:\n",
    "        products.append({\n",
    "            \"id\": product.get('data-id', \"\"),\n",
    "            \"name\": product.find('span', class_=\"name\").get_text(strip=True) if product.find('span', class_=\"name\") else \"\",\n",
    "            \"price\": product.find('span', class_=\"price\").get_text(strip=True) if product.find('span', class_=\"price\") else \"N/A\",\n",
    "            \"colors\": product.find('span', class_=\"colors\").get_text(strip=True) if product.find('span', class_=\"colors\") else \"N/A\"\n",
    "        })\n",
    "    write_json(\"Featured_Products.json\", products)\n",
    "\n",
    "# Execute all tasks\n",
    "# Calls all the above functions to perform various scraping tasks\n",
    "extract_text_data()\n",
    "extract_table_data()\n",
    "extract_product_information()\n",
    "extract_form_details()\n",
    "extract_links_and_multimedia()\n",
    "extract_featured_products()\n",
    "\n",
    "# Prints a success message when all tasks are complete\n",
    "print(\"Web scraped successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94c583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
